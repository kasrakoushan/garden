<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="In Why I am not an effective altruist, Erik Hoel criticizes the philosophical core of the EA movement. The problem with effective altruism‚Äîwhich is basically utilitarianism‚Äîis that it leads to repugnant conclusions (coined by Derek Parfit in Reasons and Persons):"><title>Contra Harris Contra Hoel on Consequentialism</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/ico href=https://brain.kasra.io//favicon.ico><link href=https://brain.kasra.io/styles.e8e77022292050a5aaf5747585c00921.min.css rel=stylesheet><link href=https://brain.kasra.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://brain.kasra.io/js/darkmode.053392c9a87cca806a2a8453e7e4afdb.min.js></script>
<script src=https://brain.kasra.io/js/util.6f22941e242efae60fd84e7c32e874fa.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script async src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script async src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script async src=https://brain.kasra.io/js/popover.f03552ccb84d99ca615d1cfb9abde59e.min.js></script>
<script defer src=https://brain.kasra.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://brain.kasra.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://brain.kasra.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://brain.kasra.io/",fetchData=Promise.all([fetch("https://brain.kasra.io/indices/linkIndex.fb2848214e545f17f13eec9a6e4ffbe4.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://brain.kasra.io/indices/contentIndex.aa54563fb761199470459b167cc9c6bf.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://brain.kasra.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://brain.kasra.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'‚Äô':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/brain.kasra.io\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-70417739-2","auto"),ga("send","pageview"))</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://brain.kasra.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://brain.kasra.io/>ü™¥ Kasra's Digital Garden</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Contra Harris Contra Hoel on Consequentialism</h1><p class=meta>Last updated
Jan 2, 2023</p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><ol><li><a href=#sam-harriss-criticism>Sam Harris‚Äôs criticism</a></li></ol></li></ol></nav></details></aside><p>In
<a href=https://erikhoel.substack.com/p/why-i-am-not-an-effective-altruist rel=noopener>Why I am not an effective altruist</a>, Erik Hoel criticizes the philosophical core of the EA movement. The problem with effective altruism‚Äîwhich is basically utilitarianism‚Äîis that it leads to repugnant conclusions (coined by Derek Parfit in <em>Reasons and Persons</em>):</p><ul><li>Example: strict utilitarianism would claim that a surgeon, trying to save ten patients with organ failure, should find someone in a back alley, murder them, and harvest all their organs.</li><li>Example: utilitarianism would claim that there there is some number of &ldquo;hiccups eliminated&rdquo; that would justify feeding a little girl to sharks.</li></ul><p>For a lot of utilitarian thought experiments, the initial experiment has a clear conclusion (e.g. you should switch the trolley so fewer people die), but then all the variations of it no longer have a clear conclusion, because morality is not mathematics.</p><p>The core mistake of utilitarianism is that it assumes that <em>the moral value of all experience is a well-ordered set</em>‚Äîthat you can line them all up on a single scale, and thus all you need to do to figure out which action is better is some addition and multiplication. In a
<a href="https://erikhoel.substack.com/p/we-owe-the-future-but-why?utm_source=substack&utm_medium=email" rel=noopener>follow up post</a>, Hoel adds: the mistake of utilitarianism is the view that morality is <em>fungible</em>, that moral value can be measured as mounds of dirt. But in reality, moral actions are not fungible.</p><p>(Note: the fact that moral values are not fungible does not preclude us from making comparisons between actions, e.g. a stubbed toe and the holocaust are entirely different <em>categories of evils</em>, but we can still compare them and say the latter is worse. Hoel‚Äôs point, though, is that not all moral actions can be compared to each other in an order-preserving manner.)</p><p>One way to avoid repugnant conclusions is to add more and more parameters to your utilitarian calculus. Hoel claims this is futile, because you&rsquo;ll never have enough parameters. (Popper had a term for this: adding
<a href=/notes/ad-hoc-hypothesis/ rel=noopener class=internal-link data-src=/notes/ad-hoc-hypothesis/>ad hoc hypotheses</a>).</p><blockquote><p>Just as how Ptolemy accounted for the movements of the planets in his geocentric model by adding in epicycles (wherein planets supposedly circling Earth also completed their own smaller circles), and this trick allowed him to still explain the occasional paradoxical movement of the planets in a fundamentally flawed geocentric model, so too does the utilitarian add moral epicycles to keep from constantly arriving at immoral outcomes.</p></blockquote><p>Hoel thinks the EA movement has done a lot of good; he just disagrees with its philosophical underpinnings. And it still leads to some repugnant-ish conclusions, e.g. that you should just be a stock broker to make more money and donate it.</p><blockquote><p>One can see repugnant conclusions pop up in the everyday choices of the movement: would you be happier being a playwright than a stock broker? Who cares, stock brokers make way more money, go make a bunch of money to give to charity. And so effective altruists have to come up with a grab bag of diluting rules to keep the repugnancy of utilitarianism from spilling over into their actions and alienating people.</p></blockquote><a href=#sam-harriss-criticism><h3 id=sam-harriss-criticism><span class=hanchor arialabel=Anchor># </span>Sam Harris‚Äôs criticism</h3></a><p>Hoel made a recent appearance on the Sam Harris podcast, in which he and Harris kept butting heads on the same basic point. Harris: all moral theories, even those that are not consequentialism, ultimately make their claims to moral value based on <em>consequences</em>. If you ask a deontologist why they advocate for some principle or other, their argument will be framed in terms of the <em>consequences</em> of people following that principle. Same thing with virtue ethics, or any other moral system.</p><p>For Harris, all the criticisms that Hoel makes still boil down to consequences. It&rsquo;s all just more consequences!</p><p>This is technically true, but at some point it begins to sound vacuous. You could describe every moral theorem in terms of eventual consequences, but it&rsquo;s not especially useful to do so. It&rsquo;s like saying &ldquo;It&rsquo;s all about goodness! Morality is all about goodness!‚Äù Hoel made this point in his piece: once you strip utilitarianism enough to no longer do strict mathematics around hedonic units, and instead just say &ldquo;do the most good you can, where good is defined in a loose, complex, personal way&rdquo;, you‚Äôve arrived at something no one can disagree with.</p><p>Choosing a moral theory requires us to answer the question of <em>how best to think</em> about <em>how we should act</em>. And it just seems that if you think about morality in terms of <em>maximization</em>, you are far more likely to engage in morally questionable behavior than if you just thought about it in terms of, say, <em>adhering to principles</em>, or <em>cultivating virtues</em>. Who are the people we think of as ethical heroes, as role models in the moral life? Gandhi, Mandela, Jesus, the Buddha. Are they <em>rationalistic maximizers</em>? Or are they just, really principled and virtuous people?</p><p>There is one place where I do agree with Harris, and it&rsquo;s around preferring human consciousness over artificial consciousness. Hoel claims that a pitfall of consequentialism is that it doesn&rsquo;t give us any reason to prefer our own survival over, say, the survival of some alien or artificial intelligence. And to that I would say: if we <em>really</em> come to the conclusion that artificial intelligences can have inner lives as rich and as laden with suffering and happiness as our own, and if they are as concerned with ethics as we are, then why <em>should</em> we prefer our wellbeing over theirs?</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/ data-ctx="Contra Harris contra Hoel on consequentialism" data-src=/ class=internal-link>Kasra's Digital Garden</a></li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://brain.kasra.io/js/graph.2d9e48dbe7ea47c0ef1c58296ce14448.js></script></div></div><div id=contact_buttons><footer><p>Made by Kasra using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, ¬© 2023</p><ul><li><a href=https://brain.kasra.io/>Home</a></li><li><a href=https://twitter.com/kasratweets>Twitter</a></li></ul></footer></div></div></body></html>